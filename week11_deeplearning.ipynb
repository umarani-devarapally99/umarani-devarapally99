{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c15c5bf3",
   "metadata": {},
   "source": [
    "# Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186fc58",
   "metadata": {},
   "source": [
    "Transformer models refer to a type of deep learning architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. The Transformer architecture has since become a foundation for many state-of-the-art natural language processing (NLP) and machine learning models.\n",
    "\n",
    "Key characteristics of Transformer models include:\n",
    "\n",
    "Self-Attention Mechanism:\n",
    "Transformers use a self-attention mechanism that allows the model to weigh different parts of the input sequence differently when making predictions. This mechanism allows the model to capture dependencies between words, even when they are far apart in the input sequence.\n",
    "\n",
    "No Recurrent or Convolutional Layers:\n",
    "Unlike earlier sequence-to-sequence models, Transformers do not rely on recurrent layers. Instead, they process the entire sequence in parallel. This parallelization leads to more efficient training and allows for better capturing long-range dependencies.\n",
    "\n",
    "Multi-Head Attention:\n",
    "The self-attention mechanism is usually implemented with multiple attention heads, allowing the model to attend to different parts of the input sequence simultaneously. This enables the model to capture various aspects of the input data.\n",
    "\n",
    "Positional Encoding:\n",
    "Since Transformers do not inherently understand the order of the elements in a sequence, positional encoding is added to the input embeddings to provide information about the position of each token in the sequence.\n",
    "\n",
    "Feedforward Neural Networks:\n",
    "Transformers include feedforward neural networks as part of their architecture to process the information captured by the attention mechanism.\n",
    "\n",
    "Layer Normalization and Residual Connections:\n",
    "Each sub-layer in a Transformer block is followed by layer normalization and a residual connection, aiding in the stability and training of deep networks.\n",
    "\n",
    "Encoder-Decoder Structure:\n",
    "Transformers are often used in an encoder-decoder structure for sequence-to-sequence tasks like machine translation or summarization. The encoder processes the input sequence, and the decoder generates the output sequence.\n",
    "\n",
    "Transfer Learning:\n",
    "Many pre-trained Transformer models are available, allowing for transfer learning. These models are initially trained on large datasets and fine-tuned for specific downstream tasks.\n",
    "\n",
    "Popular Transformer architectures include BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), T5 (Text-to-Text Transfer Transformer), and more. These models have achieved state-of-the-art results across a wide range of NLP tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9280dc9",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babadb1",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are a class of neural network architectures designed for sequential data processing. Unlike feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain a hidden state that captures information about previous inputs in the sequence. This makes them well-suited for tasks involving sequences, such as time series prediction, language modeling, and more.\n",
    "\n",
    "Key features of RNNs include:\n",
    "\n",
    "Sequential Processing:\n",
    "RNNs process input data sequentially, one element at a time, while maintaining a hidden state that captures information about previous inputs.\n",
    "\n",
    "Hidden State:\n",
    "The hidden state in an RNN serves as a memory that retains information about the previous elements in the sequence. This hidden state is updated at each time step and influences the prediction at the current time step.\n",
    "\n",
    "Vanishing and Exploding Gradients:\n",
    "Training deep RNNs can be challenging due to the vanishing and exploding gradient problems. These issues arise when gradients either become too small, causing the model to have difficulty learning long-term dependencies, or too large, leading to unstable training.\n",
    "\n",
    "Types of RNNs:\n",
    "Various types of RNN architectures exist, such as simple RNNs, Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs). LSTMs and GRUs are designed to address the vanishing gradient problem and improve the capture of long-term dependencies.\n",
    "\n",
    "Bidirectional RNNs:\n",
    "Bidirectional RNNs process the input sequence in both forward and backward directions, combining information from past and future elements in the sequence.\n",
    "\n",
    "Applications:\n",
    "RNNs are commonly used in tasks such as natural language processing (NLP), speech recognition, time series analysis, and more.\n",
    "Despite their effectiveness for some tasks, traditional RNNs have limitations in capturing long-range dependencies, and training deep RNNs can be computationally expensive. For this reason, more advanced architectures like Transformers have gained popularity in recent years for tasks involving sequential data.\n",
    "\n",
    "It's important to note that newer architectures, such as Transformers, have largely surpassed RNNs in performance for many natural language processing tasks due to their ability to capture long-range dependencies more effectively and their parallelization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67ff03",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef3240",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture designed to address the vanishing gradient problem associated with traditional RNNs. LSTMs were introduced by Hochreiter and Schmidhuber in 1997 and have become a popular choice for sequential data processing tasks due to their ability to capture long-range dependencies.\n",
    "\n",
    "Key features of LSTM networks include:\n",
    "\n",
    "Memory Cells:\n",
    "LSTMs introduce a memory cell as a fundamental building block. The memory cell is responsible for storing information over long sequences, allowing LSTMs to capture dependencies over extended time intervals.\n",
    "\n",
    "Gates:\n",
    "LSTMs use gates to control the flow of information into and out of the memory cell. The gates consist of an input gate, a forget gate, and an output gate.\n",
    "Input Gate: Regulates the flow of new information into the memory cell.\n",
    "Forget Gate: Controls the removal or retention of information from the memory cell.\n",
    "Output Gate: Determines the information to be output from the memory cell.\n",
    "\n",
    "Activation Function:\n",
    "LSTMs use activation functions to control the information flow. The hyperbolic tangent (tanh) function is often employed to squish values between -1 and 1.\n",
    "\n",
    "Long-Term Dependencies:\n",
    "LSTMs are designed to capture and remember information over long sequences, making them suitable for tasks with dependencies spread out over time.\n",
    "\n",
    "Vanishing Gradient:\n",
    "The architecture of LSTMs mitigates the vanishing gradient problem, allowing the model to learn and retain information over many time steps during training.\n",
    "\n",
    "Applications:\n",
    "LSTMs are widely used in natural language processing tasks, time series prediction, speech recognition, and any other tasks involving sequential data.\n",
    "\n",
    "While LSTMs have proven effective in capturing long-range dependencies, it's worth noting that more recent architectures, such as Transformers, have gained popularity and demonstrated superior performance in various tasks. Transformers, with their self-attention mechanism, allow for parallelization and efficient modeling of dependencies, often outperforming LSTMs in natural language processing and sequence-to-sequence tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41b0f9",
   "metadata": {},
   "source": [
    "# The evolution from RNNs and LSTMs to Transformer models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312f8c2",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) were not invented by a single person but have evolved over time through contributions from multiple researchers. One of the earliest works related to recurrent networks is attributed to Alexey Grigorevich Ivakhnenko and Valentin Grigorʹevich Lapa. They introduced the concept of \"group method of data handling\" (GMDH) in the 1960s, which had elements resembling the recurrent structure.\n",
    "\n",
    "However, the modern formulation of RNNs, particularly the backpropagation through time (BPTT) algorithm for training, is often credited to several researchers. The development of the BPTT algorithm for training RNNs is associated with Paul Werbos, who introduced the idea of backpropagation through time in his Ph.D. thesis in 1988.\n",
    "\n",
    "Despite these early contributions, training deep and recurrent networks faced challenges, including the vanishing gradient problem, which limited the effective training of RNNs on long sequences. It was later works and innovations, such as the introduction of Long Short-Term Memory (LSTM) networks by Sepp Hochreiter and Jürgen Schmidhuber in 1997, that addressed some of these challenges and made training deep RNNs more practical.\n",
    "\n",
    "In summary, the development of recurrent networks, including RNNs and their training algorithms, involved the contributions of multiple researchers over several decades. The work on addressing issues like vanishing gradients and improving the training of deep recurrent networks laid the foundation for the subsequent evolution of sequence modeling with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035c6a0",
   "metadata": {},
   "source": [
    "The Long Short-Term Memory (LSTM) network is a type of recurrent neural network (RNN) architecture designed to address the vanishing gradient problem associated with traditional RNNs. Here is a brief overview of the evolution of LSTMs:\n",
    "\n",
    "Introduction of RNNs:\n",
    "Recurrent Neural Networks (RNNs) were introduced to handle sequential data by maintaining a hidden state that captures information from previous time steps. However, traditional RNNs faced challenges with training on long sequences due to the vanishing gradient problem.\n",
    "\n",
    "Vanishing Gradient Problem:\n",
    "The vanishing gradient problem occurs when gradients diminish exponentially as they are backpropagated through time during training. This makes it difficult for the network to learn long-range dependencies in sequential data.\n",
    "\n",
    "Introduction of LSTMs:\n",
    "In 1997, Sepp Hochreiter and Jürgen Schmidhuber introduced Long Short-Term Memory (LSTM) networks. LSTMs are a type of RNN designed to mitigate the vanishing gradient problem. They incorporate memory cells and gating mechanisms to selectively store and retrieve information over long sequences.\n",
    "\n",
    "Key Components of LSTMs:\n",
    "LSTMs have three key components: the cell state, an input gate, and an output gate.\n",
    "Cell State: This represents the long-term memory of the network.\n",
    "Input Gate: Regulates the flow of information into the cell state.\n",
    "Output Gate: Controls the output based on the current input and the cell state.\n",
    "\n",
    "Gated Recurrent Unit (GRU):\n",
    "Following LSTMs, Gated Recurrent Units (GRUs) were introduced as a simplified version of LSTMs with fewer parameters. GRUs also have gating mechanisms to control information flow but with a simpler structure.\n",
    "\n",
    "Advancements and Variants:\n",
    "Over time, researchers proposed various modifications and enhancements to LSTMs, including peephole connections, layer normalization, and attention mechanisms.\n",
    "\n",
    "Widespread Adoption:\n",
    "LSTMs and their variants became widely adopted for various sequence-to-sequence tasks, including machine translation, speech recognition, and text generation.\n",
    "\n",
    "Transformer Architecture:\n",
    "While LSTMs and GRUs were effective, the Transformer architecture, introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, represented a significant shift in sequence modeling. Transformers replaced recurrence with self-attention mechanisms, allowing for parallelization and capturing long-range dependencies more effectively.\n",
    "\n",
    "\n",
    "In summary, the evolution of LSTMs played a crucial role in addressing the challenges of training deep recurrent networks on sequential data. The subsequent development of transformer models further revolutionized sequence modeling and became the dominant architecture for various natural language processing tasks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2a46f",
   "metadata": {},
   "source": [
    "# Discuss the limitations of RNNs and LSTMs and how Transformer models address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9bba83",
   "metadata": {},
   "source": [
    "### Limitations of RNNs and LSTMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0563634",
   "metadata": {},
   "source": [
    "Vanishing Gradient Problem:\n",
    "Both RNNs and LSTMs suffer from the vanishing gradient problem. Gradients diminish exponentially as they are backpropagated through time, making it challenging for the network to capture long-term dependencies in sequential data.\n",
    "\n",
    "Inability to Capture Long-Range Dependencies:\n",
    "RNNs have a limited capacity to capture long-range dependencies in sequences. This limitation hinders their performance on tasks that require understanding context over extended distances.\n",
    "\n",
    "Sequential Computation and Parallelization:\n",
    "RNNs process sequences sequentially, which limits parallelization during training. This makes them computationally inefficient and slows down the training process.\n",
    "\n",
    "Difficulty in Capturing Global Context:\n",
    "LSTMs, while addressing the vanishing gradient problem to some extent, may still struggle to capture global context effectively. They process information sequentially, and the inherent structure of recurrence may limit their ability to consider the entire input sequence simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199a394",
   "metadata": {},
   "source": [
    "###  How Transformer Models Address These Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e1d9a",
   "metadata": {},
   "source": [
    "Self-Attention Mechanism:\n",
    "Transformers use a self-attention mechanism that allows each position in the input sequence to attend to all positions simultaneously. This enables the model to capture long-range dependencies more effectively compared to the sequential processing in RNNs and LSTMs.\n",
    "\n",
    "Parallelization:\n",
    "Transformers allow for parallelization during training, as the self-attention mechanism enables the model to process all positions in parallel. This leads to faster training times compared to the sequential nature of RNNs and LSTMs.\n",
    "\n",
    "No Vanishing Gradient Problem:\n",
    "Transformers do not suffer from the vanishing gradient problem since they do not rely on sequential processing. The self-attention mechanism provides a direct path for gradient flow, allowing the model to learn dependencies across long sequences.\n",
    "\n",
    "Positional Encoding:\n",
    "Transformers incorporate positional encoding to provide information about the position of tokens in the input sequence. This helps the model maintain the order of the sequence, addressing one of the shortcomings of self-attention mechanisms.\n",
    "\n",
    "Scalability:\n",
    "Transformers are highly scalable to handle longer sequences and larger datasets. This scalability is advantageous for tasks requiring the processing of extensive contextual information.\n",
    "\n",
    "Capturing Global Context Efficiently:\n",
    "The self-attention mechanism enables transformers to capture global context efficiently. Each position can attend to all other positions, allowing the model to weigh the importance of different parts of the input sequence for each position.\n",
    "In summary, transformers address the limitations of RNNs and LSTMs by introducing a self-attention mechanism that enables parallel processing and effective capturing of long-range dependencies. The architecture of transformers has proven highly successful in various natural language processing tasks and has become the dominant model for sequence-to-sequence tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed2e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umara\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 19s 49ms/step - loss: 0.5935 - accuracy: 0.6599 - val_loss: 0.4801 - val_accuracy: 0.7706\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.3480 - accuracy: 0.8525 - val_loss: 0.4133 - val_accuracy: 0.8292\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.2010 - accuracy: 0.9246 - val_loss: 0.4807 - val_accuracy: 0.7888\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 0.0847 - accuracy: 0.9749 - val_loss: 0.5306 - val_accuracy: 0.8048\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.0327 - accuracy: 0.9923 - val_loss: 0.6230 - val_accuracy: 0.8030\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.6225 - accuracy: 0.8075\n",
      "RNN Test Accuracy: 0.807479977607727\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Load the IMDB dataset\n",
    "vocab_size = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "max_length = 100\n",
    "x_train = pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_length)\n",
    "\n",
    "# Define the RNN model\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length))\n",
    "model_rnn.add(SimpleRNN(units=32))\n",
    "model_rnn.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_rnn.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model_rnn.evaluate(x_test, y_test)\n",
    "print(f\"RNN Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3968d7",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef449811",
   "metadata": {},
   "source": [
    "\n",
    "Architecture: The RNN model consists of an Embedding layer, a SimpleRNN layer, and a Dense layer with a sigmoid activation function.\n",
    "\n",
    "Training Performance:\n",
    "Training accuracy increases significantly over epochs, reaching around 99.23%.\n",
    "Validation accuracy hovers around 80.30% after five epochs.\n",
    "\n",
    "Test Performance:\n",
    "The RNN achieves a test accuracy of approximately 80.75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ee8bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 33s 91ms/step - loss: 0.4532 - accuracy: 0.7839 - val_loss: 0.3438 - val_accuracy: 0.8536\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 28s 89ms/step - loss: 0.2663 - accuracy: 0.8977 - val_loss: 0.3454 - val_accuracy: 0.8504\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 0.1968 - accuracy: 0.9273 - val_loss: 0.3770 - val_accuracy: 0.8440\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 0.1556 - accuracy: 0.9446 - val_loss: 0.4247 - val_accuracy: 0.8338\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 27s 87ms/step - loss: 0.1253 - accuracy: 0.9561 - val_loss: 0.4952 - val_accuracy: 0.8336\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4988 - accuracy: 0.8299\n",
      "LSTM Test Accuracy: 0.8299199938774109\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Define the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length))\n",
    "model_lstm.add(LSTM(units=32))\n",
    "model_lstm.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the LSTM model\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_lstm, test_acc_lstm = model_lstm.evaluate(x_test, y_test)\n",
    "print(f\"LSTM Test Accuracy: {test_acc_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a3479",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTM) Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2cc13",
   "metadata": {},
   "source": [
    "Architecture: The LSTM model shares a similar architecture with the RNN but replaces the SimpleRNN layer with an LSTM layer.\n",
    "\n",
    "Training Performance:\n",
    "Training accuracy improves over epochs, reaching 95.61% after five epochs.\n",
    "Validation accuracy stabilizes around 83.36%.\n",
    "\n",
    "Test Performance:\n",
    "The LSTM achieves a test accuracy of approximately 82.99%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fcb22",
   "metadata": {},
   "source": [
    "###  Discussion and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13501ecb",
   "metadata": {},
   "source": [
    "RNN vs. LSTM:\n",
    "The LSTM outperforms the basic RNN, achieving higher accuracy on both training and validation sets. LSTMs are better equipped to capture long-range dependencies, making them more suitable for sequence modeling tasks.\n",
    "\n",
    "Overfitting:\n",
    "Both models show signs of overfitting, especially seen in the large gap between training and validation accuracies. Regularization techniques like dropout or early stopping could be employed to mitigate overfitting.\n",
    "\n",
    "Test Performance:\n",
    "The LSTM performs slightly better on the test set, indicating its ability to generalize better to unseen data.\n",
    "\n",
    "Model Complexity:\n",
    "The complexity of the LSTM model allows it to capture more intricate patterns in the data. However, this complexity may lead to longer training times.\n",
    "\n",
    "Improvements:\n",
    "Hyperparameter tuning, regularization, and experimenting with different model architectures could further enhance performance.\n",
    "\n",
    "\n",
    "In summary, while both RNN and LSTM models provide reasonable accuracy for sentiment analysis, the LSTM model, with its ability to capture long-term dependencies, demonstrates better performance. Further optimizations can be explored to improve generalization and mitigate overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7227fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 9s 139ms/step - loss: 0.6937 - accuracy: 0.4699 - val_loss: 0.6930 - val_accuracy: 0.5134\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 4s 110ms/step - loss: 0.6252 - accuracy: 0.7623 - val_loss: 0.9448 - val_accuracy: 0.5248\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 4s 112ms/step - loss: 0.2636 - accuracy: 0.9369 - val_loss: 1.2248 - val_accuracy: 0.5551\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.2248 - accuracy: 0.5551\n",
      "LSTM with Attention - Loss: 1.2248315811157227, Accuracy: 0.5551000237464905\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Attention, Concatenate\n",
    "import numpy as np\n",
    "\n",
    "# Define your input shape and vocab size\n",
    "input_shape = 100\n",
    "vocab_size = 10000\n",
    "\n",
    "# Placeholder data (replace with your actual data)\n",
    "x_train = np.random.randint(0, vocab_size, size=(1000, input_shape))\n",
    "y_train = np.random.randint(0, 2, size=(1000, 1))\n",
    "x_val = np.random.randint(0, vocab_size, size=(200, input_shape))\n",
    "y_val = np.random.randint(0, 2, size=(200, 1))\n",
    "\n",
    "# Implementing LSTM with Attention\n",
    "embedding_dim = 32\n",
    "lstm_units = 64\n",
    "\n",
    "# Input layer\n",
    "inputs = tf.keras.Input(shape=(input_shape,))\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_shape)(inputs)\n",
    "\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(lstm_units, return_sequences=True)(embedding_layer)\n",
    "\n",
    "# Attention mechanism\n",
    "attention = Attention()([lstm_layer, lstm_layer])\n",
    "\n",
    "# Concatenate the attention output with the LSTM output\n",
    "merged = Concatenate(axis=-1)([lstm_layer, attention])\n",
    "\n",
    "# Dense layer for classification\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# Create the model\n",
    "lstm_attention_model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "lstm_attention_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_attention_model.fit(x_train, y_train, epochs=3, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_attention_loss, lstm_attention_accuracy = lstm_attention_model.evaluate(x_val, y_val)\n",
    "print(f\"LSTM with Attention - Loss: {lstm_attention_loss}, Accuracy: {lstm_attention_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b42ff8",
   "metadata": {},
   "source": [
    "#### Discuss how the attention mechanism impacts the model's performance and its ability to handle long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1fa3b",
   "metadata": {},
   "source": [
    "The attention mechanism significantly impacts a model's performance, particularly in handling long-range dependencies. Let's delve into the key aspects of this impact:\n",
    "\n",
    "Selective Information Processing:\n",
    "Benefit in Contextual Understanding: Attention mechanisms allow the model to selectively focus on different parts of the input sequence when making predictions. This is beneficial as it enables the model to give more weight to relevant information, leading to improved contextual understanding.\n",
    "\n",
    "Handling Long-Range Dependencies:\n",
    "Addressing Vanishing Gradient Problem: Traditional recurrent neural networks (RNNs) may struggle with capturing dependencies over long sequences due to the vanishing gradient problem. Attention mechanisms mitigate this issue by allowing the model to assign higher weights to relevant parts of the sequence, enabling the capture of long-range dependencies.\n",
    "\n",
    "Improved Memory and Context Integration:\n",
    "Contextual Information Retrieval: Attention mechanisms enable the model to retrieve relevant contextual information from different parts of the input sequence. This is crucial for tasks where understanding the entire context is necessary for accurate predictions. The model can effectively integrate information from distant time steps.\n",
    "\n",
    "Enhanced Model Performance:\n",
    "Increased Accuracy: By focusing on specific elements of the input sequence, attention mechanisms enhance the model's ability to make accurate predictions. This is particularly valuable in tasks such as sequence-to-sequence translation, where certain words in the source language may have a strong influence on the translation.\n",
    "\n",
    "Interpretable Models:\n",
    "Visualization of Attention Weights: Attention mechanisms provide interpretability by allowing the visualization of attention weights. This transparency allows practitioners to understand which parts of the input sequence are crucial for decision-making at different steps. Interpretability is valuable for model debugging and building trust in model predictions.\n",
    "\n",
    "Adaptability to Varying Sequence Lengths:\n",
    "Dynamic Handling of Sequence Lengths: Attention mechanisms adapt well to varying sequence lengths. Unlike fixed-size approaches, attention mechanisms dynamically adjust the focus based on the input, making them more robust to sequences of different lengths. This adaptability is particularly relevant in natural language processing tasks with variable-length sentences.\n",
    "\n",
    "Computational Complexity:\n",
    "Increased Computational Cost: Attention mechanisms come with increased computational costs, especially as the sequence length grows. The computation of attention weights for each element in the sequence can be computationally intensive. Techniques like scaled dot-product attention are commonly used to manage this computational complexity.\n",
    "\n",
    "\n",
    "In summary, attention mechanisms enhance a model's ability to capture dependencies across different parts of a sequence, leading to improved performance and a better understanding of contextual information. However, practitioners need to consider the computational costs associated with attention mechanisms, especially for large datasets and sequences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2f73e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\umara\\anaconda3\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\umara\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\umara\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eb084e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 35s 35s/step - loss: 10.0151 - accuracy: 0.1364\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 1s 969ms/step - loss: 9.9059 - accuracy: 0.1364\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 1s 886ms/step - loss: 10.8431 - accuracy: 0.1364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14591d0a130>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder data (replace with your actual data)\n",
    "source_texts = [\"I love natural language processing.\", \"This is a transformer model example.\"]\n",
    "target_texts = [\"J'adore le traitement du langage naturel.\", \"Ceci est un exemple de modèle de transformer.\"]\n",
    "\n",
    "# Tokenize input and target texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenized_inputs = tokenizer(source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "tokenized_targets = tokenizer(target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Ensure both input_ids and decoder_input_ids are provided during training\n",
    "model_inputs = {\n",
    "    \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "    \"decoder_input_ids\": tokenized_targets[\"input_ids\"],\n",
    "}\n",
    "\n",
    "# Load pre-trained T5 model\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(model_inputs, tokenized_targets[\"input_ids\"], epochs=3, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8df43a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step - loss: 10.3775 - accuracy: 0.1111\n",
      "Evaluation Loss: [10.377483367919922, 0.1111111119389534]\n"
     ]
    }
   ],
   "source": [
    "# Placeholder evaluation data (replace with your actual evaluation data)\n",
    "evaluation_source_texts = [\"This is another example.\", \"Translate this sentence.\"]\n",
    "evaluation_target_texts = [\"Ceci est un autre exemple.\", \"Traduisez cette phrase.\"]\n",
    "\n",
    "# Tokenize evaluation input and target texts\n",
    "tokenized_evaluation_inputs = tokenizer(evaluation_source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "tokenized_evaluation_targets = tokenizer(evaluation_target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Ensure both input_ids and decoder_input_ids are provided during evaluation\n",
    "evaluation_inputs = {\n",
    "    \"input_ids\": tokenized_evaluation_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_evaluation_inputs[\"attention_mask\"],\n",
    "    \"decoder_input_ids\": tokenized_evaluation_targets[\"input_ids\"],\n",
    "}\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_loss = model.evaluate(evaluation_inputs, tokenized_evaluation_targets[\"input_ids\"])\n",
    "print(f\"Evaluation Loss: {evaluation_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "079db3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 46s 46s/step - loss: 10.8245 - accuracy: 0.1364\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.1871 - accuracy: 0.1818\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.6720 - accuracy: 0.1364\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.0909\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 973ms/step - loss: 10.5510 - accuracy: 0.1818\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 973ms/step - loss: 10.3775 - accuracy: 0.0909\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 996ms/step - loss: 10.3775 - accuracy: 0.0909\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 956ms/step - loss: 10.3775 - accuracy: 0.1364\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 965ms/step - loss: 10.3775 - accuracy: 0.1364\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.5966 - accuracy: 0.1364\n",
      "1/1 [==============================] - 7s 7s/step - loss: 10.3775 - accuracy: 0.1111\n",
      "Evaluation Loss: [10.377483367919922, 0.1111111119389534]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder data (replace with your actual data)\n",
    "source_texts = [\"I love natural language processing.\", \"This is a transformer model example.\"]\n",
    "target_texts = [\"J'adore le traitement du langage naturel.\", \"Ceci est un exemple de modèle de transformer.\"]\n",
    "\n",
    "# Tokenize input and target texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenized_inputs = tokenizer(source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "tokenized_targets = tokenizer(target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Ensure both input_ids and decoder_input_ids are provided during training\n",
    "model_inputs = {\n",
    "    \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "    \"decoder_input_ids\": tokenized_targets[\"input_ids\"],\n",
    "}\n",
    "\n",
    "# Load T5-small model\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Model Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)  # Experiment with learning rate\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model Training\n",
    "model.fit(model_inputs, tokenized_targets[\"input_ids\"], epochs=10, batch_size=5)  # Experiment with batch size and epochs\n",
    "\n",
    "# Model Evaluation\n",
    "evaluation_source_texts = [\"This is another example.\", \"Translate this sentence.\"]\n",
    "evaluation_target_texts = [\"Ceci est un autre exemple.\", \"Traduisez cette phrase.\"]\n",
    "tokenized_evaluation_inputs = tokenizer(evaluation_source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "tokenized_evaluation_targets = tokenizer(evaluation_target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "evaluation_inputs = {\n",
    "    \"input_ids\": tokenized_evaluation_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_evaluation_inputs[\"attention_mask\"],\n",
    "    \"decoder_input_ids\": tokenized_evaluation_targets[\"input_ids\"],\n",
    "}\n",
    "\n",
    "evaluation_loss = model.evaluate(evaluation_inputs, tokenized_evaluation_targets[\"input_ids\"])\n",
    "print(f\"Evaluation Loss: {evaluation_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a08cdfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1/1 [==============================] - 46s 46s/step - loss: 11.1319 - accuracy: 0.0909\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.7301 - accuracy: 0.0909\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.0909\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.1364\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.5897 - accuracy: 0.0909\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.8643 - accuracy: 0.1364\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.0909\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.1364\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.6386 - accuracy: 0.0909\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.1818\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.1364\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.4301 - accuracy: 0.1818\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.6386 - accuracy: 0.0909\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.8608 - accuracy: 0.1364\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.0909\n",
      "WARNING:tensorflow:5 out of the last 32 calls to <function Model.make_test_function.<locals>.test_function at 0x00000145A6655CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 7s 7s/step - loss: 10.3775 - accuracy: 0.1111\n",
      "Evaluation Loss: [10.377483367919922, 0.1111111119389534]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Placeholder data (replace with your actual data)\n",
    "source_texts = [\"I love natural language processing.\", \"This is a transformer model example.\"]\n",
    "target_texts = [\"J'adore le traitement du langage naturel.\", \"Ceci est un exemple de modèle de transformer.\"]\n",
    "\n",
    "# Tokenize input and target texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenized_inputs = tokenizer(source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "tokenized_targets = tokenizer(target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Ensure both input_ids and decoder_input_ids are provided during training\n",
    "model_inputs = {\n",
    "    \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "    \"decoder_input_ids\": tokenized_targets[\"input_ids\"],\n",
    "}\n",
    "\n",
    "# Load T5-small model\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Model Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)  # Experiment with learning rate\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model Training\n",
    "model.fit(model_inputs, tokenized_targets[\"input_ids\"], epochs=15, batch_size=8)  # Experiment with batch size and epochs\n",
    "\n",
    "# Model Evaluation\n",
    "evaluation_source_texts = [\"This is another example.\", \"Translate this sentence.\"]\n",
    "evaluation_target_texts = [\"Ceci est un autre exemple.\", \"Traduisez cette phrase.\"]\n",
    "tokenized_evaluation_inputs = tokenizer(evaluation_source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "tokenized_evaluation_targets = tokenizer(evaluation_target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "evaluation_inputs = {\n",
    "    \"input_ids\": tokenized_evaluation_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_evaluation_inputs[\"attention_mask\"],\n",
    "    \"decoder_input_ids\": tokenized_evaluation_targets[\"input_ids\"],\n",
    "}\n",
    "\n",
    "evaluation_loss = model.evaluate(evaluation_inputs, tokenized_evaluation_targets[\"input_ids\"])\n",
    "print(f\"Evaluation Loss: {evaluation_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95163a",
   "metadata": {},
   "source": [
    "### I tired changing hyperparameters for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49bf9f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1/1 [==============================] - 46s 46s/step - loss: 10.3775 - accuracy: 0.1250\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.0000e+00\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.0833\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.1004 - accuracy: 0.0833\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.9074 - accuracy: 0.0417\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.5537 - accuracy: 0.0833\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.1250\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.1250\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.6607 - accuracy: 0.0833\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.6168 - accuracy: 0.0417\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.2368 - accuracy: 0.1250\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.0833\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3775 - accuracy: 0.0833\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.3847 - accuracy: 0.0417\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.6743 - accuracy: 0.2083\n",
      "WARNING:tensorflow:6 out of the last 33 calls to <function Model.make_test_function.<locals>.test_function at 0x000001462A7373A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 8s 8s/step - loss: 10.3775 - accuracy: 0.1250\n",
      "Evaluation Loss: [10.377483367919922, 0.125]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Placeholder data (replace with your actual data)\n",
    "source_texts = [\"I love natural language processing.\", \"This is a transformer model example.\"]\n",
    "target_texts = [\"Ich liebe die Verarbeitung natürlicher Sprache.\", \"Dies ist ein Beispiel für ein Transformer-Modell.\"]\n",
    "\n",
    "# Tokenize input and target texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenized_inputs = tokenizer(source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "tokenized_targets = tokenizer(target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Ensure both input_ids and decoder_input_ids are provided during training\n",
    "model_inputs = {\n",
    "    \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "    \"decoder_input_ids\": tokenized_targets[\"input_ids\"],\n",
    "}\n",
    "\n",
    "# Load T5-small model\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Model Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)  # Experiment with learning rate\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model Training\n",
    "model.fit(model_inputs, tokenized_targets[\"input_ids\"], epochs=15, batch_size=8)  # Experiment with batch size and epochs\n",
    "\n",
    "# Model Evaluation\n",
    "evaluation_source_texts = [\"This is another example.\", \"Translate this sentence.\"]\n",
    "evaluation_target_texts = [\"Dies ist ein weiteres Beispiel.\", \"Übersetzen Sie diesen Satz.\"]\n",
    "tokenized_evaluation_inputs = tokenizer(evaluation_source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "tokenized_evaluation_targets = tokenizer(evaluation_target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "evaluation_inputs = {\n",
    "    \"input_ids\": tokenized_evaluation_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_evaluation_inputs[\"attention_mask\"],\n",
    "    \"decoder_input_ids\": tokenized_evaluation_targets[\"input_ids\"],\n",
    "}\n",
    "\n",
    "evaluation_loss = model.evaluate(evaluation_inputs, tokenized_evaluation_targets[\"input_ids\"])\n",
    "print(f\"Evaluation Loss: {evaluation_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3924669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: In recent years, natural language processing has made significant advancements.\n",
      "Summary: natural language processing has made significant progresses in recent years.\n",
      "\n",
      "Source: The field of machine learning continues to evolve rapidly.\n",
      "Summary: Der Bereich der machine learning wächst rapid, und es wächst.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sample data\n",
    "source_texts = [\n",
    "    \"In recent years, natural language processing has made significant advancements.\",\n",
    "    \"The field of machine learning continues to evolve rapidly.\",\n",
    "    # Add more source texts as needed\n",
    "]\n",
    "\n",
    "# Tokenize input texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenized_inputs = tokenizer(source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "\n",
    "# Load T5 model for summarization\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Generate summaries\n",
    "generated_ids = model.generate(\n",
    "    input_ids=tokenized_inputs[\"input_ids\"],\n",
    "    max_length=150,  # You can adjust the length based on your requirements\n",
    "    num_beams=4,     # Adjust the beam search parameters\n",
    "    length_penalty=2.0,  # Adjust the length penalty\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "# Decode and print the generated summaries\n",
    "generated_summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "for source_text, summary in zip(source_texts, generated_summaries):\n",
    "    print(f\"Source: {source_text}\")\n",
    "    print(f\"Summary: {summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371ef94",
   "metadata": {},
   "source": [
    "Comparing the performance of different models involves evaluating them on relevant metrics and analyzing the results. In the provided examples, I used two different models: T5-small and MarianMT for language translation. Here's a general approach for comparing their performance and discussing the differences:\n",
    "\n",
    "Performance Metrics:\n",
    "Evaluation Loss:Look at the evaluation loss after training for both models. Lower loss generally indicates better performance, but it's essential to consider other metrics.\n",
    "BLEU Score:Use the BLEU score for translation tasks. BLEU measures the similarity between predicted and reference translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73a7ae",
   "metadata": {},
   "source": [
    "Steps to Compare:\n",
    "Train and evaluate both models on the same dataset using the same evaluation metrics.\n",
    "Calculate BLEU scores and compare evaluation losses.\n",
    "Analyze the results and observe any patterns or differences.\n",
    "Consider the aspects mentioned above while discussing why one model performs better or differently than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd3970",
   "metadata": {},
   "source": [
    "Model Architecture:\n",
    "T5-small and MarianMT have different architectures. T5 is a versatile model designed for various NLP tasks, while MarianMT is specifically tailored for machine translation.\n",
    "\n",
    "Training Data:\n",
    "Consider the size and quality of the training data. More diverse and extensive datasets can contribute to better generalization.\n",
    "\n",
    "Hyperparameters:\n",
    "Evaluate the impact of hyperparameters such as learning rate, batch size, and the number of training epochs. Different models may respond differently to hyperparameter settings.\n",
    "\n",
    "Tokenization and Input Representation:\n",
    "Transformers rely on tokenization, and the choice of tokenizer can influence performance. Ensure consistency in tokenization across models.\n",
    "\n",
    "Fine-Tuning vs. Pre-trained Models:\n",
    "The T5-small model was fine-tuned on your specific translation task, while MarianMT is a pre-trained translation model. Fine-tuning might lead to better task-specific performance.\n",
    "\n",
    "Model Size:\n",
    "Consider the size of the models. Smaller models may be faster but might sacrifice performance compared to larger counterparts.\n",
    "\n",
    "Multilingual vs. Task-Specific:\n",
    "MarianMT is specifically designed for translation tasks, whereas T5 is a more general-purpose model. Task-specific models might outperform more generalized models on their designated tasks.\n",
    "\n",
    "Language Peculiarities:\n",
    "Consider the peculiarities of the source and target languages. Some models may perform better on specific language pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa0e21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
